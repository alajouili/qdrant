import os
from qdrant_client import QdrantClient
from transformers import CLIPProcessor, CLIPModel
import torch

# --- 1. CONNEXION Ã€ LA BASE EXISTANTE ---
dossier_script = os.path.dirname(os.path.abspath(__file__))
os.chdir(dossier_script)

print("ğŸš€ Connexion Ã  Qdrant...")
client = QdrantClient(path="./ma_base_qdrant")
COLLECTION_NAME = "mes_photos"

# --- 2. CHARGEMENT IA ---
print("ğŸ§  Chargement du cerveau (CLIP)...")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# --- 3. INTERACTION UTILISATEUR ---
print("\n" + "="*40)
texte_recherche = input("âœï¸  Que cherches-tu dans tes photos ? (ex: un chien, une voiture...) : ")
print("="*40 + "\n")

# --- 4. VECTORISATION DU TEXTE ---
# On transforme ta phrase en mathÃ©matiques
inputs = processor(text=[texte_recherche], return_tensors="pt", padding=True)

with torch.no_grad():
    text_features = model.get_text_features(**inputs)
    # Conversion en liste pour Qdrant
    query_vector = text_features.detach().numpy()[0].tolist()

# --- 5. RECHERCHE ---
hits = client.search(
    collection_name=COLLECTION_NAME,
    query_vector=query_vector,
    limit=3
)

print(f"ğŸ” RÃ©sultats pour '{texte_recherche}' :")
found = False
for hit in hits:
    if hit.score > 0.2: # On filtre les rÃ©sultats trop faibles
        found = True
        print(f"ğŸ“¸ TrouvÃ© : {hit.payload['filename']}  (Score de ressemblance : {hit.score:.3f})")

if not found:
    print("âŒ Aucune image correspondante trouvÃ©e (Score trop faible).")